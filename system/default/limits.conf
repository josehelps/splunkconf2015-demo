#   Version 6.2.6
# DO NOT EDIT THIS FILE!
# Changes to default files will be lost on update and are difficult to
# manage and support.
#
# Please make any changes to system defaults by overriding them in
# apps or $SPLUNK_HOME/etc/system/local  
# (See "Configuration file precedence" in the web documentation).
#
# To override a specific setting, copy the name of the stanza and
# setting to the file where you wish to override it.
#
# This file configures various limits to the Splunk's search commands.
# CAUTION: Do not alter the settings in limits.conf unless you know what you are doing. 
#
# Improperly configured limits may result in splunkd crashes and/or memory overuse.


[searchresults]
maxresultrows = 50000
# maximum number of times to try in the atomic write operation (1 = no retries)
tocsv_maxretry = 5
# retry period is 1/2 second (500 milliseconds)
tocsv_retryperiod_ms = 500

compression_level = 1

[search_info]
# These setting control logging of error messages to info.csv
# All messages will be logged to search.log regardless of these settings.
# maximum number of error messages to log in info.csv
# Set to 0 to remove limit, may affect search performance
max_infocsv_messages  = 20
# log level = DEBUG | INFO | WARN | ERROR
infocsv_log_level = INFO
 

[subsearch]
# maximum number of results to return from a subsearch
maxout = 10000
# maximum number of seconds to run a subsearch before finalizing
maxtime = 60
# time to cache a given subsearch's results
ttl = 300

[lookup]
# maximum size of static lookup file to use a in-memory index for
max_memtable_bytes = 10000000
# maximum matches for a lookup
max_matches = 1000
# maximum reverse lookup matches (for search expansion)
max_reverse_matches = 50
# default setting for if non-memory file lookups (for large files) should batch queries
# can be override via a lookup table's stanza in transforms.conf
batch_index_query = true
# when doing batch request, what's the most matches to retrieve
# if more than this limit of matches would otherwise be retrieve, we will fall back to non-batch mode matching
batch_response_limit = 5000000
# maximum number of lookup error messages that should be logged
max_lookup_messages = 20
 

[anomalousvalue]
maxresultrows = 50000
# maximum number of distinct values for a field
maxvalues = 0
# maximum size in bytes of any single value (truncated to this size if larger)
maxvaluesize = 0

[associate]
maxfields = 10000
maxvalues = 0
maxvaluesize = 0

[autoregress]
maxp = 10000
maxrange = 1000

[concurrency]
# maximum concurrency level to keep record of
max_count = 10000000

# for the contingency, ctable, and counttable commands
[ctable]
maxvalues = 1000

[correlate]
maxfields = 1000

# for bin/bucket/discretize
[discretize]
maxbins = 50000 
# if maxbins not specified or = 0, defaults to searchresults::maxresultrows

[inputcsv]
# maximum number of retries for creating a tmp directory (with random name in SPLUNK_HOME/var/run/splunk)
mkdir_max_retries = 100

[indexpreview]
# maximum number of bytes to read from each file during preview
max_preview_bytes = 2000000
# maximum number of results to emit per call to preview data generator
max_results_perchunk = 2500
# loosely-applied maximum on number of preview data objects held in memory
soft_preview_queue_size = 100


[inputproc]
# Threshold size (in mb) to trigger fishbucket rolling to a new db
file_tracking_db_threshold_mb = 500


[join]
subsearch_maxout = 50000
subsearch_maxtime = 60
subsearch_timeout = 120

[kmeans]
maxdatapoints = 100000000
maxkvalue = 1000
maxkrange = 100

[typeahead]
maxcount          = 1000
fetch_multiplier  = 50
use_cache         = true
cache_ttl_sec     = 300
min_prefix_length = 1
max_concurrent_per_user = 3

[kv]
# when non-zero, the point at which kv should stop creating new columns
maxcols  = 512
# maximum number of keys auto kv can generate
limit    = 50
# truncate _raw to to this size and then do auto KV
maxchars = 10240

max_extractor_time = 1000
avg_extractor_time = 500

[metadata]
maxresultrows = 10000
# the most metadata results to fetch from each indexer.
maxcount = 100000

[metrics]
# the number of series to include in the per_x_thruput reports in metrics.log
maxseries = 10

[metrics:tcpin_connections]
# keep each connection metrics
aggregate_metrics = false
# keep _tcp_Bps, _tcp_KBps, _tcp_avg_thruput, _tcp_Kprocessed that can be derived from kb
suppress_derived_info = false

[rare]
maxresultrows = 50000
# maximum distinct value vectors to keep track of
maxvalues = 0
maxvaluesize = 0

[restapi]
# maximum result rows to be return by /events or /results getters from REST API  
maxresultrows = 50000

# regex constraint on time_format and output_time_format for search endpoints
time_format_reject = [<>!]

# truncate the properties over this length in the contents dictionary of a job entry from the jobs endpoint
# 0 means don't truncate
jobscontentmaxcount = 0

[search_metrics]
# Add more detail to the per-search metrics 
debug_metrics = false

[search]
search_process_mode = auto

# Corresponds to the size of the results queue in the dispatch fetch level
result_queue_max_size = 100000000
# Defaults to download all remote logs other than saved search logs and oneshot search logs
fetch_remote_search_log = disabledSavedSearches

# use precomputed summaries if possible?
summary_mode = all

# max length of custom job id when passing spawning new job
max_id_length = 150

# how long searches should be stored on disk once completed
ttl = 600

# how long searches should be stored on disk once failed
failed_job_ttl = 86400

# how long should searches run for a search head live on the indexers
remote_ttl = 600

# by default, no timeline information is retained.  UI will supply the status_buckets as needed
status_buckets = 0

# the maximum number of end results to store globally (when status_buckets=0,1)
max_count = 500000

# truncate report output to max_count?
truncate_report = false

# the minimum length of a prefix before a * to ask the index about
min_prefix_len = 1

# the length of time to persist search cache entries (in seconds)
cache_ttl = 300

# maximum results per call to search (in dispatch), must be <= maxresultrows
max_results_perchunk = 2500

# minimum reuslts per call to search (in dispatch), must be <= max_results_perchunk
min_results_perchunk = 100

# maximum raw size of results per call to search (in dispatch), 0 = no limit, not affected by chunk_multiplier
max_rawsize_perchunk = 100000000

# target duration of a particular call to fetch search results in ms
target_time_perchunk = 2000

# time in seconds until a search is considered "long running"
long_search_threshold = 2

# max_results_perchunk, min_results_perchunk, and target_time_perchunk 
# are multiplied by this for a long running search
chunk_multiplier = 5

# the minimum frequency of a field displayed in the /summary endpoint
min_freq = 0.01

# the frequency with which try to reduce intermediate data when there is an non-streaming and non-stateful streaming command. (0 = never)
reduce_freq = 10

# the maximum time to spend doing reduce, as a fraction of total search time
reduce_duty_cycle = 0.25

# the maximum time to spend generating previews, as a fraction of total search time
preview_duty_cycle = 0.25

# the minimum number of results blobs to keep for consumption by the search head.
results_queue_min_size = 10

# the maximum number of times to retry to dispatch a search when the quota has been reached
dispatch_quota_retry = 4

# milliseconds between retrying to dispatch a search if a quota has been reached
# we retry the given number of times, with each successive wait 2x longer than the previous
dispatch_quota_sleep_ms = 100

# the maximum number of concurrent searches per CPU 
max_searches_per_cpu = 1

# the base number of concurrent searches
base_max_searches = 6

# max real-time searches = max_rt_search_multiplier x max historical searches
max_rt_search_multiplier = 1

# the total number of concurrent searches is base_max_searches + #cpus*max_searches_per_cpu

# max recursion depth for macros 
# considered a search exception if macro expansion doesn't stop after this many levels
max_macro_depth = 100

# for real-time searches in the UI, maximum number of events stored (as a FIFO buffer)
realtime_buffer = 10000

# stack size of the search executing thread
stack_size = 4194304

# the number of search job metadata to cache in RAM
status_cache_size = 10000

# search results combiner maximum in-memory buffer size (in events)
max_combiner_memevents = 50000

# the minimum bundle replication period
replication_period_sec  = 60

# bundle replication file ttl
replication_file_ttl = 600

# whether bundle replication is synchronous (and thus blocking searches)
sync_bundle_replication = auto

# whether to use multiple threads when setting up distributed search to multiple peers
multi_threaded_setup = 0

# when doing round-robin fetching, what are the min,max,and backoff factors for the amount
# of time (in ms) to sleep when no data is available
rr_min_sleep_ms = 10
rr_max_sleep_ms = 1000
rr_sleep_factor = 2

# how often to update the field summary statistics, as a ratio to the elapsed run time so far
fieldstats_update_freq = 0
# maximum period for updating field summary statistics in seconds
fieldstats_update_maxperiod = 60

# allow timeline to be map/reduced?
remote_timeline = true
# minimum peers required to utlize remote timelining
remote_timeline_min_peers = 1
# how often to touch remote artifacts to keep them from being reaped when search has not finished? (in seconds)
remote_timeline_touchperiod = 300
# whether to fetch all events accessible through the timeline from the remote peers before the job is considered done
remote_timeline_fetchall = 1

# timeouts for fetching remote timeline events
remote_timeline_connection_timeout = 5
remote_timeline_send_timeout = 10
remote_timeline_receive_timeout = 10

# default setting for allowing async jobs to be queued if quota violation
default_allow_queue = true

# how often to retry queued jobs (in seconds)
queued_job_check_freq = 1

# enable search history?
enable_history = true

# max number of searches to store in history (per user/app)
max_history_length = 1000

# allow inexact metasearch?
allow_inexact_metasearch = false

dispatch_dir_warning_size = 2000

# track indextime range of searches (shown in job inspector)
track_indextime_range = true

# avoid loading remote bundles in splunkd
load_remote_bundles = false

# allow batch mode which searches in non-time order for certain classes of searches
allow_batch_mode = true

# when in batch mode what is the max number of index values to read in at one time
batch_search_max_index_values = 10000000

# When batch mode attempts to retry the search on a peer that failed wait at least this many seconds
batch_retry_min_interval = 5

# When batch mode attempts to retry the search on a peer that failed wait at most this many seconds
batch_retry_max_interval = 300

# After a retry attempt fails increase the time to wait before trying again by this scaling factor
batch_retry_scaling = 1.5

# If all currently active peers have finished with the search wait this many seconds before giving up on
# peers we are attempting to reconnect to for a retry.
batch_wait_after_end = 900

# how long jobs are saved for by default
default_save_ttl = 604800

#do we write multi file results to results_dir
write_multifile_results_out = true

#maximum number of worker threads in Round Robin policy
max_workers_searchparser = 5

#maximum size of the chunk queue
max_chunk_queue_size = 1000000

#absolute value of largest timeskew we will tolerate between the searchead and the peer (in seconds)
max_tolerable_skew = 60

[realtime] 
# default options for indexer support of real-time searches
# these can all be overriden for a single search via REST API arguments

# size of queue for each real-time search
queue_size = 10000

# should indexer block if a queue is full?
blocking = false

# maximum time to block if the queue is full (meaningless if blocking = false)
max_blocking_secs = 60

# should the indexer prefilter events for efficiency?
indexfilter = true

# should real-time windowed searches backfill with historical data by default?
default_backfill = true

# should real-time windowed searches sort events to be in descending time order?
enforce_time_order = true

# should we use indexedRealtime by default
indexed_realtime_use_by_default = false

# number of seconds to wait for disk flushes to finish with indexed/continuous/psuedo realtime search
indexed_realtime_disk_sync_delay = 60

# minimum seconds to wait between component index searches during an indexed realtime search
indexed_realtime_default_span = 1

# max number of seconds allowed to fall behind realtime before we drop data 
# and reset back to the default span from realtime.
indexed_realtime_maximum_span = 0

# frequency to fetch updated bucket list from daemon
indexed_realtime_cluster_update_interval = 30

# this limits the frequency that we will trigger alerts during a realtime search
alerting_period_ms = 0

[set]
maxresultrows = 50000


[slc]
# maximum number of clusters to create
maxclusters = 10000

[findkeywords]
maxevents = 50000

[sort]
# maximum number of concurrent files to open
maxfiles = 64

[spath]
# number of characters to read from an XML or JSON event when auto extracting
extraction_cutoff = 5000
extract_all = true

[stats]
maxresultrows = 50000
maxvalues = 0
maxvaluesize = 0
# for streamstats's maximum window size
max_stream_window = 10000
# for rdigest, used to approximate order statistics (median, percentiles)
rdigest_k = 100
rdigest_maxnodes = 1

[sistats]
maxvalues = 0
maxvaluesize = 0
rdigest_k = 100
rdigest_maxnodes = 1
max_valuemap_bytes = 100000

[top]
maxresultrows = 50000
# maximum distinct value vectors to keep track of
maxvalues = 0
maxvaluesize = 0


[transactions]
# maximum number of open transaction or events in open
# transaction before transaction eviction happens
maxopentxn    = 5000
maxopenevents = 100000


[typer]
# in eventtyping, pay attention to first N characters of any
# attribute (e.g., _raw), including individual tokens. Can be
# overridden by supplying the typer operator with the argument 
# maxlen (e.g. "|typer maxlen=300").
maxlen = 10000


[authtokens]
# Time before an auth token will be expire if not used, in seconds.
# 0 will indicate no timeout.
expiration_time = 3600

[sample]
maxsamples = 10000
maxtotalsamples = 100000


[scheduler]
# the maximum number of searches the scheduler can run, as a percentage
# of the maximum number of concurrent searches 
max_searches_perc  = 50

# fraction of concurrent scheduler searches to use for auto summarization
auto_summary_perc  = 50 

# maximum number of results to load when triggering an action
max_action_results = 50000

action_execution_threads  = 2

actions_queue_size        = 100

actions_queue_timeout     = 30

alerts_max_count          = 50000 

alerts_max_history        = 7d

alerts_scoping            = host

alerts_expire_period      = 120

persistance_period         = 30

# maximum number of lock files to keep around for each scheduled search
# effective only if search head pooling is enabled, the most recent files are kept
max_lock_files = 5

# the lock file reaper should clean lock files that are this old (in seconds)
max_lock_file_ttl = 86400

max_per_result_alerts = 500

scheduled_view_timeout = 60m

# Scheduler timeout for printing a throttled warning message
# if we're hitting scheduler concurrency limits
concurrency_message_throttle_time = 10m

# by default the scheduler should not run jobs on itself in search head pooling mode
# it should dispatch to pool members.
shp_dispatch_to_slave = true

[auto_summarizer]
cache_timeout      = 600
maintenance_period = 1800
return_actions_with_normalized_ids = fromcontext
normalized_summaries = true
detailed_dashboard = true
shc_accurate_access_counts = false
# by default disabling the cache for saved-search metadata.
enable_saved_searches_cache = false

[thruput]
# throughput limiting at index time
maxKBps = 0

[show_source]
# maximum events retriveable by show source
max_count = 10000
max_timebefore = 1day
max_timeafter = 1day
distributed = true
# maximum events we will request in the distributed show source. Likely all of these will not be used
distributed_search_limit = 30000

[ldap]
# maximum number of users we will attempt to precache from LDAP after reloading auth
max_users_to_precache = 1000
# controls whether we allow login when we find multiple entries with the same value for the username attribute
allow_multiple_matching_users = true

[reversedns]
# max percent of time allowed for reverse dns lookups for incoming 
# forwarder connections before WARN is logged in splunkd.log
# sanity check diagnostic for slow lookups
rdnsMaxDutyCycle = 10

[viewstates]
# is the viewstate reaper enabled?
enable_reaper = true
# how often does the reaper run?
reaper_freq = 86400
# how many viewstates does the reaper consider "acceptable"?
reaper_soft_warn_level = 1000
# reaper eligibility age
ttl = 86400

[geostats]
# at the lowest level of the tree, i.e. ZL=0 (when we are zoomed out to the world level ), what is the
# size of each gridcell in terms of latitude and longitude (degrees)? Valid values for
# zl_0_gridcell_latspan are from 0 to 180.0, and for zl_0_gridcell_longspan are from 0 to 360.0
# Rest of the zoom level gridcell sizes are auto-tuning, i.e. will reduce by a factor of 2 
# at each additional level
zl_0_gridcell_latspan = 22.5
zl_0_gridcell_longspan = 45.0
# configures the filtering/search strategy for events on the map
# currently experimental
filterstrategy = 2
# how many levels of clustering will be done in geostats.
maxzoomlevel = 9


[tscollect]
# default value of 'squashcase' arg if not specified by the command
squashcase = false
# default value of 'keepresults' arg if not specified by the command
keepresults = false
# the max allowed size of tsidx files to create in megabytes. '0' implies no limit
optimize_max_size_mb = 256

[tstats]
# whether we apply role-based search filters when users run tstats on normal index data (never applied on data from tscollect or datamodel acceleration)
apply_search_filter = true
# default value of 'summariesonly' arg if not specified by the command
summariesonly = false
# default value of 'allow_old_summaries' arg if not specified by the command
allow_old_summaries = false
# by default we retrieve up to ten million events at once from a TSIDX file when answering queries
chunk_size = 10000000

[pdf]
# the max number of rows that the pdfgen rendering engine (not PDF Report Server app) will render for any individual table or event listing
max_rows_per_table = 1000
# the number of seconds after which the pdfgen render endpoint will timeout if it has not yet finished rendering the PDF output 
render_endpoint_timeout = 3600

[kvstore]
# the max number of accelerations that can be assigned to a single collection
# Valid values range from 0 to 50
max_accelerations_per_collection = 10
# the max number of fields that can be part of an acceleration
# Valid values range from 0 to 30
max_fields_per_acceleration = 10
# the max number of rows that will be returned per query
max_rows_per_query = 50000
# the max number of queries that can be run as part of the same batch
max_queries_per_batch = 1000
# the max size of a query result in MB
max_size_per_result_mb = 50
# the max size of a batch save operation in MB
max_size_per_batch_save_mb = 50
# the max number of documents of a batch save operation
max_documents_per_batch_save = 1000
# the max size of a batched query result in MB
max_size_per_batch_result_mb = 100

